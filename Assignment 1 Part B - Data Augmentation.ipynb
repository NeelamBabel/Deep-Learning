{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Image Augmentation \n",
    "\n",
    "Image Augmentation is the process of taking images that are already in a training dataset and manipulating them to create many altered versions of the same image. This both provides more images to train on, and can also help expose our classifier to a wider variety of lighting and coloring situations so as to make our classifier more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the need of Image Augmentation ?\n",
    "\n",
    "To combat the high expense of collecting thousands of training images, image augmentation has been developed in order to generate training data from an existing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 25\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert class vectors to binary class metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Preprocessing and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the Default Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an Image using ImageDataGenerator() \n",
    "\n",
    "To Augment our image data with keras, we simply call the ImageDataGenerator() function and pass a list of parameters describing the alterations that we want it to perform on the images. We will then call the fit() function on our image generator which will apply the changes to the images batch by batch. By default, the modifications will be applied randomly, so not every image will be changed every time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None)\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        #validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2000/2000 [==============================] - 483s - loss: 1.8061 - acc: 0.3372   \n",
      "Epoch 2/25\n",
      "2000/2000 [==============================] - 480s - loss: 1.5327 - acc: 0.4412   \n",
      "Epoch 3/25\n",
      "2000/2000 [==============================] - 540s - loss: 1.3823 - acc: 0.5033   \n",
      "Epoch 4/25\n",
      "2000/2000 [==============================] - 533s - loss: 1.2947 - acc: 0.5363   \n",
      "Epoch 5/25\n",
      "2000/2000 [==============================] - 542s - loss: 1.2218 - acc: 0.5647   \n",
      "Epoch 6/25\n",
      "2000/2000 [==============================] - 509s - loss: 1.1560 - acc: 0.5921   \n",
      "Epoch 7/25\n",
      "2000/2000 [==============================] - 459s - loss: 1.1109 - acc: 0.6069   \n",
      "Epoch 8/25\n",
      "2000/2000 [==============================] - 506s - loss: 1.0657 - acc: 0.6230   \n",
      "Epoch 9/25\n",
      "2000/2000 [==============================] - 495s - loss: 1.0253 - acc: 0.6389   \n",
      "Epoch 10/25\n",
      "2000/2000 [==============================] - 534s - loss: 0.9910 - acc: 0.6519   \n",
      "Epoch 11/25\n",
      "2000/2000 [==============================] - 473s - loss: 0.9669 - acc: 0.6606   \n",
      "Epoch 12/25\n",
      "2000/2000 [==============================] - 455s - loss: 0.9405 - acc: 0.6682   \n",
      "Epoch 13/25\n",
      "2000/2000 [==============================] - 488s - loss: 0.9233 - acc: 0.6753   \n",
      "Epoch 14/25\n",
      "2000/2000 [==============================] - 523s - loss: 0.9030 - acc: 0.6857   \n",
      "Epoch 15/25\n",
      "2000/2000 [==============================] - 485s - loss: 0.8888 - acc: 0.6923   \n",
      "Epoch 16/25\n",
      "2000/2000 [==============================] - 516s - loss: 0.8703 - acc: 0.6978   \n",
      "Epoch 17/25\n",
      "2000/2000 [==============================] - 513s - loss: 0.8655 - acc: 0.7005   \n",
      "Epoch 18/25\n",
      "2000/2000 [==============================] - 514s - loss: 0.8550 - acc: 0.7043   \n",
      "Epoch 19/25\n",
      "2000/2000 [==============================] - 515s - loss: 0.8404 - acc: 0.7086   \n",
      "Epoch 20/25\n",
      "2000/2000 [==============================] - 578s - loss: 0.8395 - acc: 0.7091   \n",
      "Epoch 21/25\n",
      "2000/2000 [==============================] - 461s - loss: 0.8352 - acc: 0.7136   \n",
      "Epoch 22/25\n",
      "2000/2000 [==============================] - 456s - loss: 0.8177 - acc: 0.7201   \n",
      "Epoch 23/25\n",
      "2000/2000 [==============================] - 469s - loss: 0.8188 - acc: 0.7197   \n",
      "Epoch 24/25\n",
      "2000/2000 [==============================] - 464s - loss: 0.8130 - acc: 0.7211   \n",
      "Epoch 25/25\n",
      "2000/2000 [==============================] - 463s - loss: 0.8117 - acc: 0.7222   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11a5fe780>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
    "                    steps_per_epoch= 2000, epochs= 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /Users/neelambabel/Desktop/Cognitive Computing/saved_models/keras_cifar10_trained_model.h5 \n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.691914773178\n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the base mdoel, we were able to achieve an accuracy of 72% on training data and 69% on test data. Let us see the impact of changing the Image Generator parameters on accuracy in the cases below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1 : Rotating the images by 60 Degrees and Hz and vertical flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=60,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=True,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None)\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        #validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2000/2000 [==============================] - 483s - loss: 1.5591 - acc: 0.4439   \n",
      "Epoch 2/25\n",
      "2000/2000 [==============================] - 498s - loss: 1.4716 - acc: 0.4749   \n",
      "Epoch 3/25\n",
      "2000/2000 [==============================] - 467s - loss: 1.4424 - acc: 0.4891   \n",
      "Epoch 4/25\n",
      "2000/2000 [==============================] - 475s - loss: 1.4124 - acc: 0.5000   \n",
      "Epoch 5/25\n",
      "2000/2000 [==============================] - 468s - loss: 1.3967 - acc: 0.5055   \n",
      "Epoch 6/25\n",
      "2000/2000 [==============================] - 473s - loss: 1.3832 - acc: 0.5105   \n",
      "Epoch 7/25\n",
      "2000/2000 [==============================] - 480s - loss: 1.3742 - acc: 0.5162   \n",
      "Epoch 8/25\n",
      "2000/2000 [==============================] - 513s - loss: 1.3598 - acc: 0.5204   \n",
      "Epoch 9/25\n",
      "2000/2000 [==============================] - 483s - loss: 1.3476 - acc: 0.5246   \n",
      "Epoch 10/25\n",
      "2000/2000 [==============================] - 512s - loss: 1.3473 - acc: 0.5267   \n",
      "Epoch 11/25\n",
      "2000/2000 [==============================] - 507s - loss: 1.3433 - acc: 0.5257   \n",
      "Epoch 12/25\n",
      "2000/2000 [==============================] - 497s - loss: 1.3378 - acc: 0.5301   \n",
      "Epoch 13/25\n",
      "2000/2000 [==============================] - 489s - loss: 1.3374 - acc: 0.5302   \n",
      "Epoch 14/25\n",
      "2000/2000 [==============================] - 487s - loss: 1.3260 - acc: 0.5328   \n",
      "Epoch 15/25\n",
      "2000/2000 [==============================] - 517s - loss: 1.3303 - acc: 0.5337   \n",
      "Epoch 16/25\n",
      "2000/2000 [==============================] - 460s - loss: 1.3150 - acc: 0.5389   \n",
      "Epoch 17/25\n",
      "2000/2000 [==============================] - 484s - loss: 1.3276 - acc: 0.5347   \n",
      "Epoch 18/25\n",
      "2000/2000 [==============================] - 480s - loss: 1.3143 - acc: 0.5382   \n",
      "Epoch 19/25\n",
      "2000/2000 [==============================] - 463s - loss: 1.3184 - acc: 0.5370   \n",
      "Epoch 20/25\n",
      "2000/2000 [==============================] - 476s - loss: 1.3128 - acc: 0.5411   \n",
      "Epoch 21/25\n",
      "2000/2000 [==============================] - 478s - loss: 1.3194 - acc: 0.5384   \n",
      "Epoch 22/25\n",
      "2000/2000 [==============================] - 483s - loss: 1.3177 - acc: 0.5389   \n",
      "Epoch 23/25\n",
      "2000/2000 [==============================] - 501s - loss: 1.3151 - acc: 0.5421   \n",
      "Epoch 24/25\n",
      "2000/2000 [==============================] - 549s - loss: 1.3143 - acc: 0.5398   \n",
      "Epoch 25/25\n",
      "2000/2000 [==============================] - 478s - loss: 1.3116 - acc: 0.5416   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12228fcc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
    "                    steps_per_epoch= 2000, epochs= 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /Users/neelambabel/Desktop/Cognitive Computing/saved_models/keras_cifar10_trained_model.h5 \n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 1.04107242508\n",
      "Test accuracy: 0.6404\n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is underfitting as test data accuracy is high as compared to training data, so training more samples would help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2 : Setting featurewise std normalization = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None)\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        #validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2000/2000 [==============================] - 536s - loss: 1.1857 - acc: 0.6023   \n",
      "Epoch 2/25\n",
      "2000/2000 [==============================] - 533s - loss: 1.0774 - acc: 0.6336   \n",
      "Epoch 3/25\n",
      "2000/2000 [==============================] - 533s - loss: 1.0403 - acc: 0.6515   \n",
      "Epoch 4/25\n",
      "2000/2000 [==============================] - 537s - loss: 1.0175 - acc: 0.6581   \n",
      "Epoch 5/25\n",
      "2000/2000 [==============================] - 540s - loss: 1.0078 - acc: 0.6629   \n",
      "Epoch 6/25\n",
      "2000/2000 [==============================] - 529s - loss: 0.9932 - acc: 0.6670   \n",
      "Epoch 7/25\n",
      "2000/2000 [==============================] - 604s - loss: 0.9799 - acc: 0.6727   \n",
      "Epoch 8/25\n",
      "2000/2000 [==============================] - 652s - loss: 0.9699 - acc: 0.6743   \n",
      "Epoch 9/25\n",
      "2000/2000 [==============================] - 494s - loss: 0.9695 - acc: 0.6769   \n",
      "Epoch 10/25\n",
      "2000/2000 [==============================] - 644s - loss: 0.9638 - acc: 0.6789   \n",
      "Epoch 11/25\n",
      "2000/2000 [==============================] - 578s - loss: 0.9687 - acc: 0.6784   \n",
      "Epoch 12/25\n",
      "2000/2000 [==============================] - 533s - loss: 0.9607 - acc: 0.6807   \n",
      "Epoch 13/25\n",
      "2000/2000 [==============================] - 536s - loss: 0.9509 - acc: 0.6845   \n",
      "Epoch 14/25\n",
      "2000/2000 [==============================] - 537s - loss: 0.9607 - acc: 0.6820   \n",
      "Epoch 15/25\n",
      "2000/2000 [==============================] - 535s - loss: 0.9605 - acc: 0.6798   \n",
      "Epoch 16/25\n",
      "2000/2000 [==============================] - 541s - loss: 0.9561 - acc: 0.6831   \n",
      "Epoch 17/25\n",
      "2000/2000 [==============================] - 533s - loss: 0.9578 - acc: 0.6847   \n",
      "Epoch 18/25\n",
      "2000/2000 [==============================] - 535s - loss: 0.9628 - acc: 0.6824   \n",
      "Epoch 19/25\n",
      "2000/2000 [==============================] - 540s - loss: 0.9559 - acc: 0.6845   \n",
      "Epoch 20/25\n",
      "2000/2000 [==============================] - 541s - loss: 0.9619 - acc: 0.6816   \n",
      "Epoch 21/25\n",
      "2000/2000 [==============================] - 542s - loss: 0.9643 - acc: 0.6832   \n",
      "Epoch 22/25\n",
      "2000/2000 [==============================] - 537s - loss: 0.9629 - acc: 0.6823   \n",
      "Epoch 23/25\n",
      "2000/2000 [==============================] - 536s - loss: 0.9698 - acc: 0.6813   \n",
      "Epoch 24/25\n",
      "2000/2000 [==============================] - 539s - loss: 0.9640 - acc: 0.6825   \n",
      "Epoch 25/25\n",
      "2000/2000 [==============================] - 540s - loss: 0.9680 - acc: 0.6831   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122890438>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
    "                    steps_per_epoch= 2000, epochs= 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /Users/neelambabel/Desktop/Cognitive Computing/saved_models/keras_cifar10_trained_model.h5 \n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 1.59539349689\n",
      "Test accuracy: 0.4594\n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to be overfitting, hence it is better to train the model on more parameters to reduce the gap in train and test accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
